{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness Pipeline Development Toolkit - Demonstration\n",
    "\n",
    "This notebook demonstrates the complete end-to-end fairness pipeline using a real-world loan approval dataset.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Installation](#setup)\n",
    "2. [Understanding the Pipeline](#architecture)\n",
    "3. [Data Preparation](#data)\n",
    "4. [Running the Pipeline](#execution)\n",
    "5. [Analyzing Results](#results)\n",
    "6. [MLflow Tracking](#mlflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "### Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import mlflow\n",
    "\n",
    "from src.run_pipeline import PipelineOrchestrator\n",
    "\n",
    "print(\"All dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding the Pipeline\n",
    "\n",
    "The pipeline executes in the following sequence:\n",
    "\n",
    "1. **Configuration Loading**: Validates config.yml using Pydantic\n",
    "2. **Data Loading**: Loads loan approval dataset and creates train/test split\n",
    "3. **Baseline Measurement**: Calculates initial fairness metrics\n",
    "4. **Preprocessing**: Applies bias mitigation transformations\n",
    "5. **Model Training**: Trains model with fairness constraints\n",
    "6. **Final Validation**: Evaluates fairness improvements\n",
    "7. **MLflow Logging**: Records all artifacts and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "### Load Sample Dataset\n",
    "\n",
    "This demonstration uses a loan approval dataset containing:\n",
    "- **Features**: loan_amount, income, credit_score, employment_status\n",
    "- **Protected Attributes**: gender, race, age_group\n",
    "- **Target**: loan_approved (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('../data/loan_approval.csv')\n",
    "\n",
    "if data_path.exists():\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    display(df.head())\n",
    "    print(f\"\\nTarget distribution:\")\n",
    "    print(df['loan_approved'].value_counts(normalize=True))\n",
    "else:\n",
    "    print(f\"Dataset not found at {data_path}\")\n",
    "    print(\"Please ensure the dataset is available before running the pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Current Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../config.yml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Current Pipeline Configuration:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Data Source: {config['data']['path']}\")\n",
    "print(f\"Target Column: {config['data']['target_column']}\")\n",
    "print(f\"Protected Attributes: {config['data']['protected_attributes']}\")\n",
    "print(f\"\\nPreprocessing: {config['preprocessing']['transformers']}\")\n",
    "print(f\"Repair Level: {config['preprocessing']['repair_level']}\")\n",
    "print(f\"\\nTraining Method: {config['training']['method']}\")\n",
    "print(f\"Fairness Constraint: {config['training']['constraint']}\")\n",
    "print(f\"\\nPrimary Metric: {config['fairness']['primary_metric']}\")\n",
    "print(f\"Threshold: {config['fairness']['threshold']}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running the Pipeline\n",
    "\n",
    "### Execute Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orchestrator = PipelineOrchestrator(config_path=\"../config.yml\")\n",
    "orchestrator.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyzing Results\n",
    "\n",
    "### Extract Pipeline Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_metrics = orchestrator.baseline_metrics\n",
    "final_metrics = orchestrator.final_metrics\n",
    "\n",
    "print(\"Pipeline Execution Results\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nBaseline Metrics:\")\n",
    "for metric, value in baseline_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nFinal Metrics:\")\n",
    "for metric, value in final_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_metric = config['fairness']['primary_metric']\n",
    "threshold = config['fairness']['threshold']\n",
    "\n",
    "comparison_data = {\n",
    "    'Metric': [primary_metric, 'accuracy', 'precision', 'recall'],\n",
    "    'Baseline': [\n",
    "        baseline_metrics.get(primary_metric, 0),\n",
    "        0,\n",
    "        0,\n",
    "        0\n",
    "    ],\n",
    "    'Final': [\n",
    "        final_metrics.get(primary_metric, 0),\n",
    "        final_metrics.get('accuracy', 0),\n",
    "        final_metrics.get('precision', 0),\n",
    "        final_metrics.get('recall', 0)\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "df_comparison['Improvement'] = df_comparison['Baseline'] - df_comparison['Final']\n",
    "\n",
    "print(\"\\nFairness-Performance Analysis:\")\n",
    "display(df_comparison)\n",
    "\n",
    "final_fairness = final_metrics.get(primary_metric, float('inf'))\n",
    "if final_fairness <= threshold:\n",
    "    print(f\"\\nSUCCESS: Fairness metric ({final_fairness:.4f}) is within threshold ({threshold:.4f})\")\n",
    "else:\n",
    "    print(f\"\\nALERT: Fairness metric ({final_fairness:.4f}) exceeds threshold ({threshold:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Fairness Improvement', 'Model Performance')\n",
    ")\n",
    "\n",
    "fairness_baseline = baseline_metrics.get(primary_metric, 0)\n",
    "fairness_final = final_metrics.get(primary_metric, 0)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=['Baseline', 'Final', 'Threshold'],\n",
    "        y=[fairness_baseline, fairness_final, threshold],\n",
    "        marker_color=['red', 'green', 'blue'],\n",
    "        text=[f'{fairness_baseline:.3f}', f'{fairness_final:.3f}', f'{threshold:.3f}'],\n",
    "        textposition='auto'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "performance_metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "performance_values = [final_metrics.get(m, 0) for m in performance_metrics]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=performance_metrics,\n",
    "        y=performance_values,\n",
    "        marker_color='lightblue',\n",
    "        text=[f'{v:.3f}' for v in performance_values],\n",
    "        textposition='auto'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Pipeline Results Dashboard\",\n",
    "    showlegend=False,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. MLflow Tracking\n",
    "\n",
    "### Query MLflow Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = config['mlflow']['experiment_name']\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "if experiment:\n",
    "    print(f\"Experiment: {experiment_name}\")\n",
    "    print(f\"Experiment ID: {experiment.experiment_id}\")\n",
    "    print(f\"Artifact Location: {experiment.artifact_location}\")\n",
    "    \n",
    "    runs = mlflow.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        order_by=[\"start_time DESC\"],\n",
    "        max_results=5\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nRecent Runs ({len(runs)}):\")\n",
    "    display(runs[['run_id', 'start_time', 'metrics.final_accuracy', \n",
    "                  'metrics.final_demographic_parity_difference']])\n",
    "else:\n",
    "    print(f\"Experiment '{experiment_name}' not found\")\n",
    "    print(\"Run the pipeline first to create the experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View MLflow UI\n",
    "\n",
    "To view the MLflow UI, run in terminal:\n",
    "\n",
    "```bash\n",
    "mlflow ui --port 5000\n",
    "```\n",
    "\n",
    "Then open: http://localhost:5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This demonstration showcased:\n",
    "\n",
    "- **Declarative Configuration**: YAML-based pipeline control with Pydantic validation\n",
    "- **Modular Architecture**: Clean separation of concerns across modules\n",
    "- **Automated Orchestration**: End-to-end execution with single command\n",
    "- **MLflow Integration**: Complete experiment tracking and reproducibility\n",
    "- **Real-World Application**: Loan approval use case in finance domain\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Customize configuration for your specific use case\n",
    "2. Integrate your own dataset\n",
    "3. Extend modules with custom functionality\n",
    "4. Deploy as part of MLOps pipeline\n",
    "5. Monitor fairness in production"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
